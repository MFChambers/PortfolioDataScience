---
title: "Coding Challenge - Timed"
subtitle: "This timed challenge demonstrates practical fluency with Python, pandas, Matplotlib/Seaborn, and scikit-learn under constraints."
author: "Maia Faith Chambers"
format:
  html:
    self-contained: true
    toc: true
    toc-depth: 2
    title-block-banner: true
    code-fold: true
    code-summary: "Show Code"
    code-tools:
      toggle: true
      caption: See Code
execute:
  kernel: python312
  warning: false
---

## Client Request:
Complete four timed tasks using Python to load public datasets, clean/transform values, visualize trends, and train a quick baseline ML model. The deliverable is a single Quarto report that runs end-to-end without manual steps.

# Summary of overall project work

  * Data access & validation: Loaded public CSVs (e.g., baby names) directly from GitHub; verified schema and previewed columns.
  * Visualization under time pressure: Produced quick line + bar charts with titles/labels/annotations to communicate trends at a glance.
  * Data wrangling: Parsed ranges (e.g., age bins), handled missing values (impute with statistics), and reshaped data for plotting.
  * Rapid ML baseline: One-hot encoded features and trained a Random Forest to predict a demographic attribute; reported accuracy and surfaced top feature importances.

Tools/Packages:Python 3.12, pandas, NumPy, Matplotlib, Seaborn, scikit-learn, Quarto.
```{python}
import sys, os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

print(sys.executable)
sns.set(style="whitegrid")

url = "https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv"
names = pd.read_csv(url)

print(names.columns)
names.head()

```

## Plot Name Popularity Over Time

```{python}
# Load official names dataset
url = "https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv"
names = pd.read_csv(url)

# Remove the 'Total' row and reshape
names = names[names["name"] != "Total"].copy()
long = names.melt(id_vars=["name", "year"], var_name="state", value_name="n")

# Filter for John in UT and OR
john = long[(long["name"] == "John") & (long["state"].isin(["UT", "OR"]))].copy()

# Static line chart (keeps HTML tiny vs Plotly)
plt.figure(figsize=(10, 5))
for st, color in [("UT", "red"), ("OR", "orange")]:
    sub = john[john["state"] == st]
    plt.plot(sub["year"], sub["n"], label=st, linewidth=2, color=color)

# Vertical markers
for x, y in [(1936, 400), (1976, 700), (1999, 300)]:
    plt.axvline(x, color="black", linewidth=1)
    plt.text(x + 0.5, y, str(x), fontsize=10)

plt.title("The History of 'John' for Utah (red) and Oregon (orange)")
plt.xlabel("Year name given")
plt.ylabel("Count of John")
plt.xlim(1910, 2020)
plt.legend(title="State")
plt.tight_layout()
plt.show()
```


## Impute Missing Values & Compute Summary

```{python}
problem = pd.Series([np.nan, 18, 22, 45, 31, np.nan, 85, 38, 129, 8000, 22, 2])

std_dev = problem.std()
problem_filled = problem.fillna(std_dev)
mean_result = round(problem_filled.mean(), 2)

print("Standard Deviation used for imputation:", round(std_dev, 2))
print("Mean after imputation:", mean_result)

```

## Parse Age Ranges and Visualize Distribution

```{python}
# Given age ranges
ages = pd.Series([
    "10-25", "10-25", "26-35", "56-85", "0-9", "46-55",
    "56-85", "0-9", "26-35", "56-85", "0-9", "10-25"
])

# Expand ranges (handles en-dash/hyphen robustly)
expanded = []
for rng in ages:
    # Normalize any en-dash to hyphen
    rng = rng.replace("â€“", "-").strip()
    lo, hi = rng.split("-")
    lo, hi = int(lo), int(hi)
    # Label ages <= 35 as 'young' else 'old'
    for age in range(lo, hi + 1):
        group = "young" if age <= 35 else "old"
        expanded.append({"value": age, "group": group})

df_age = pd.DataFrame(expanded)
df_age["group"] = pd.Categorical(df_age["group"], categories=["old", "young"])

plt.figure(figsize=(8, 5))
sns.boxplot(data=df_age, x="group", y="value", width=0.6, showfliers=False)
plt.xlabel("group")
plt.ylabel("age")
plt.ylim(0, 90)
plt.title("Age Distribution by Group")
plt.tight_layout()
plt.show()

```

## Quick ML Baseline + Feature Importance

```{python}
# Load cleaned Star Wars survey (public course copy)
sw_url = "http://byuistats.github.io/CSE250-Course/data/clean_starwars.csv"
sw = pd.read_csv(sw_url)

# Target: is_female
sw["gender"] = sw["gender"].astype(str).str.strip().str.lower()
sw["is_female"] = sw["gender"].map({"female": 1, "male": 0})

# Drop rows with missing target, then simple feature cleaning
sw = sw.dropna(subset=["is_female"]).copy()
y = sw["is_female"].astype(int)

# Use all other columns except the raw gender/target
X = sw.drop(columns=["gender", "is_female"])

# One-hot encode categoricals; keep numeric as-is
X = pd.get_dummies(X, drop_first=True)

# Train/test split & model
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.20, random_state=2022
)

rf = RandomForestClassifier(random_state=2022)
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)

acc = accuracy_score(y_test, y_pred)
print(f"Random Forest Accuracy: {acc:.2%}")

# Top 10 feature importances
importances = pd.Series(rf.feature_importances_, index=X.columns)
top10 = importances.sort_values(ascending=False).head(10).reset_index()
top10.columns = ["Feature", "Importance"]

plt.figure(figsize=(10, 6))
sns.barplot(data=top10, x="Importance", y="Feature")
plt.title("Top 10 Feature Importances")
plt.tight_layout()
plt.show()

# (Optional) show shapes instead of printing raw arrays
print("Train shape:", X_train.shape, " Test shape:", X_test.shape)


```