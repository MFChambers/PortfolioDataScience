{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Client Report - Can You Predict That?\"\n",
        "subtitle: \"This project builds and evaluates classification models to predict whether a home was built before 1980 using Python (pandas, NumPy, scikit-learn), Seaborn/Matplotlib, and Quarto.\"\n",
        "author: \"Maia Faith Chambers\"\n",
        "format:\n",
        "  html:\n",
        "    self-contained: true\n",
        "    page-layout: full\n",
        "    title-block-banner: true\n",
        "    toc: true\n",
        "    toc-depth: 3\n",
        "    toc-location: body\n",
        "    number-sections: false\n",
        "    html-math-method: katex\n",
        "    code-fold: true\n",
        "    code-summary: \"Show the code\"\n",
        "    code-overflow: wrap\n",
        "    code-copy: hover\n",
        "    code-tools:\n",
        "      source: false\n",
        "      toggle: true\n",
        "      caption: See code\n",
        "execute: \n",
        "  warning: false\n",
        "---\n",
        "\n",
        "\n",
        "## Client Request:\n",
        "The client is a Colorado state agency responsible for safeguarding the health and safety of residents. Many housing records in their system are missing the year built. To address this, they need a predictive model that can classify whether a home was built before 1980. This information is critical for prioritizing inspections of older homes that may contain asbestos or other health hazards.\n",
        "\n",
        "The client provided a housing dataset and requested a model that delivers a clear, explainable baseline. Specifically, they want:\n",
        "    * Visualizations that reveal key data patterns\n",
        "    * Feature importance rankings to understand predictive drivers\n",
        "    * Standard evaluation metrics (accuracy, precision, recall) to assess performance\n",
        "\n",
        "The solution must avoid data leakage (e.g., not using yrbuilt to predict before1980) and be designed so it can be extended with neighborhood features in future iterations.\n",
        "\n",
        "# Summary of overall project work\n",
        "\n",
        "    * Exploratory visuals: Compared living area, bathrooms, and stories across classes; newer homes tend to be larger, with more bathrooms and multi-story layouts.\n",
        "    * Modeling: Trained Decision Tree, Random Forest, and Logistic Regression on non-leaky features; tuned the tree (max_depth=5, min_samples_leaf=50) for interpretability.\n",
        "    * Performance: ~78‚Äì80% test accuracy; Random Forest offered the best balance of robustness and interpretability (with ranked feature importances).\n",
        "    * Feature importance: Living area was most predictive, followed by bathrooms and stories‚Äîconsistent with domain intuition.\n",
        "    * Evaluation: Reported accuracy, precision, recall, F1, and confusion matrices to show trade-offs between false positives/negatives.\n",
        "\n",
        "Additional Analysis: \n",
        "    * Leakage check: Demonstrated why including yrbuilt inflates accuracy (near 100%) and removed it; repeated with neighborhood join to show proper handling prevents leakage.\n",
        "    * Regression stretch: Predicted yrbuilt with a Random Forest Regressor (strong ùëÖ2 and reasonable RMSE), useful for estimating missing build years.\n",
        "\n",
        "Tools/Packages: pandas, NumPy, scikit-learn (DecisionTree, RandomForest, LogisticRegression, RandomForestRegressor), Seaborn/Matplotlib, Quarto.\n",
        "\n",
        "Data hygiene notes: Removed yrbuilt and parcel from classification features to prevent leakage; if using grouped/categorical features, one-hot encode them before modeling.\n"
      ],
      "id": "dc11d2c7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_graphviz\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, confusion_matrix\n",
        "import graphviz\n",
        "\n",
        "# Load dataset\n",
        "url = \"https://raw.githubusercontent.com/byuidatascience/data4dwellings/master/data-raw/dwellings_ml/dwellings_ml.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Prepare data\n",
        "df['stories_str'] = df['stories'].astype(str)\n",
        "df['numbaths_grouped'] = pd.cut(df['numbaths'], bins=[0, 1, 2, 3, 4, np.inf], labels=['<=1', '1-2', '2-3', '3-4', '4+'])\n",
        "df['livearea_grouped'] = pd.cut(df['livearea'], bins=[0, 1000, 1500, 2000, 2500, np.inf], labels=['<1000', '1000-1500', '1500-2000', '2000-2500', '2500+'])\n",
        "\n",
        "# Add labels\n",
        "df['before1980_label'] = df['before1980'].map({0: \"Built 1980+\", 1: \"Built Before 1980\"})\n",
        "df['before1980_num'] = df['before1980']\n",
        "\n",
        "# Set styling\n",
        "sns.set(style=\"whitegrid\")\n",
        "custom_palette = {\"Built 1980+\": \"#00BFC4\", \"Built Before 1980\": \"#F8766D\"}"
      ],
      "id": "ccb98d0e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Elevator pitch\n",
        "Homes built after 1980 are more likely to have larger living areas, multiple stories, and more bathrooms. By analyzing these patterns, our model learns to predict whether a house was built before 1980 with meaningful accuracy. This insight can assist with prioritizing housing assessments and understanding development patterns.\n",
        "\n",
        "## Visual Signals for Model Splits\n",
        "These visualizations show potential relationships that a machine learning model could use to split the data. For instance, the living area (Chart 1) suggests that post-1980 homes are generally larger. Bathroom count (Chart 2) shows a shift in distribution where newer homes more often include additional bathrooms. Lastly, stories (Chart 3) indicates that single-story homes may be more common in earlier decades. These patterns can serve as helpful split points in decision trees or contribute predictive value in models like random forests or logistic regression.\n"
      ],
      "id": "e856ae26"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Chart A: Grouped bathrooms\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(data=df, x='numbaths_grouped', hue='before1980_label', palette=custom_palette)\n",
        "plt.title('Grouped Number of Bathrooms vs. Year Built')\n",
        "plt.xlabel('Number of Bathrooms')\n",
        "plt.ylabel('Count')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "563a4e29",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Chart B: Living area boxplot\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.boxplot(data=df, x='before1980_label', y='livearea', palette=custom_palette)\n",
        "plt.title('Living Area by Year Built')\n",
        "plt.xlabel('Year Built Category')\n",
        "plt.ylabel('Living Area (sqft)')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "ccff4fbc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Chart C: Proportional stories\n",
        "story_prop = df.groupby(['stories_str', 'before1980_label']).size().reset_index(name='count')\n",
        "story_total = story_prop.groupby('stories_str')['count'].transform('sum')\n",
        "story_prop['proportion'] = story_prop['count'] / story_total\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(data=story_prop, x='stories_str', y='proportion', hue='before1980_label', palette=custom_palette)\n",
        "plt.title('Proportion of Story Count by Year Built')\n",
        "plt.xlabel('Number of Stories')\n",
        "plt.ylabel('Proportion')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "728d4a96",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Baseline Models and Performance\n",
        "A Decision Tree Classifier was initially selected to label homes as built before or after 1980, using the features living area (livearea), number of stories (stories), and number of bathrooms (numbaths). The model was tuned with max_depth=5 and min_samples_leaf=50 to reduce overfitting while retaining interpretability for public health staff. The resulting test accuracy was approximately 78.5%, which is a solid baseline but does not meet the 90% target.\n",
        "\n",
        "Additional models were explored:\n",
        "\n",
        "Logistic Regression: around 78.7% accuracy; limited by its linear nature.\n",
        "\n",
        "k-Nearest Neighbors (k-NN): approximately 88% accuracy, but highly sensitive to data scaling and local outliers.\n",
        "\n",
        "Random Forest: approximately 80.3% accuracy, with stronger generalization than a single tree, and offering ranked feature importances for interpretability.\n"
      ],
      "id": "364a5be0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Features\n",
        "features = df.drop(columns=['before1980_num', 'yrbuilt','parcel'])\n",
        "#features = features.fillna(0)\n",
        "\n",
        "# Target\n",
        "target = df['before1980_num']"
      ],
      "id": "ea0820d5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Decision Tree\n",
        "clf_tree = DecisionTreeClassifier(max_depth=5, min_samples_leaf=50, criterion='entropy', random_state=42)\n",
        "clf_tree.fit(X_train, y_train)\n",
        "y_pred_tree = clf_tree.predict(X_test)\n",
        "tree_acc = accuracy_score(y_test, y_pred_tree)\n",
        "print(f\"Decision Tree Accuracy: {tree_acc:.2%}\")\n"
      ],
      "id": "e7774d12",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Random Forest\n",
        "clf_rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf_rf.fit(X_train, y_train)\n",
        "y_pred_rf = clf_rf.predict(X_test)\n",
        "rf_acc = accuracy_score(y_test, y_pred_rf)\n",
        "print(f\"Random Forest Accuracy: {rf_acc:.2%}\")\n"
      ],
      "id": "a570d907",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Logistic Regression\n",
        "clf_lr = LogisticRegression(max_iter=1000)\n",
        "clf_lr.fit(X_train, y_train)\n",
        "y_pred_lr = clf_lr.predict(X_test)\n",
        "lr_acc = accuracy_score(y_test, y_pred_lr)\n",
        "print(f\"Logistic Regression Accuracy: {lr_acc:.2%}\")"
      ],
      "id": "8b76334e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "None of the models tested reached the 90% accuracy goal. However, Random Forest provides the best balance of interpretability, robustness, and predictive performance for a classification baseline. With further feature engineering ‚Äî for example, integrating neighborhood data or temporal trends ‚Äî and hyperparameter optimization (such as a grid search for tree depth and minimum leaf size), there is potential to improve model performance to approach or exceed 90% in the future.\n",
        "\n",
        "In the current scope, the Random Forest is recommended as the best candidate for a production baseline. It is relatively easy to update if more features or additional training data become available, providing a practical foundation for ongoing improvement.\n",
        "\n",
        "## Which Features Matter Most?\n",
        "Feature importance analysis revealed that living area (livearea) was the strongest predictor of whether a home was built before or after 1980. This makes sense because homes constructed after 1980 tend to follow modern architectural trends favoring more spacious floorplans, in contrast to smaller post-war homes built before stricter asbestos regulations.\n",
        "\n",
        "The second most important feature was number of bathrooms (numbaths). Newer homes typically include more bathrooms to match contemporary expectations for convenience and functionality, making bathroom count a reliable indicator of more recent construction.\n",
        "\n",
        "The number of stories (stories) feature also contributed to the classification model, although with a lower importance score. This is still valuable because single-story homes were historically more common in earlier decades, whereas modern subdivisions often include two-story designs.\n"
      ],
      "id": "7123cff6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Decision Tree importance\n",
        "importance_tree = pd.Series(clf_tree.feature_importances_, index=features.columns).sort_values()\n",
        "plt.figure(figsize=(8, 5))\n",
        "importance_tree.plot(kind='barh', color='lightseagreen', edgecolor='black')\n",
        "plt.title('Decision Tree Feature Importance')\n",
        "plt.xlabel('Importance Score')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "9d84b63a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Random Forest importance\n",
        "importance_rf = pd.Series(clf_rf.feature_importances_, index=features.columns).sort_values()\n",
        "plt.figure(figsize=(8, 5))\n",
        "importance_rf.plot(kind='barh', color='salmon', edgecolor='black')\n",
        "plt.title('Random Forest Feature Importance')\n",
        "plt.xlabel('Importance Score')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "7e711c51",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These patterns are visualized in the accompanying feature importance chart below, which shows the ranked contribution of each feature to the model. The chart confirms that living area, number of bathrooms, and number of stories are the dominant variables. Overall, these variables align with real-world domain knowledge about housing design and construction patterns and justify the model‚Äôs predictions in a way that is explainable and transparent for stakeholders.\n",
        "\n",
        "Further improvements could include adding neighborhood-level attributes or temporal price trends to enhance predictive power.\n",
        "\n",
        "## Evaluation: Precision, Recall, and Trade-offs\n",
        "I evaluated the classification models using three common metrics: accuracy, precision, and recall. Each provides a different perspective on model performance:\n",
        "\n",
        "Accuracy measures the overall proportion of correct predictions across both classes. The Random Forest achieved approximately 80% accuracy, slightly higher than the Decision Tree (78%) and Logistic Regression (79%). However, accuracy alone can be misleading if the classes are imbalanced, which is why we also examine precision and recall.\n"
      ],
      "id": "5afafd7c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Decision Tree Evaluation:\")\n",
        "print(classification_report(y_test, y_pred_tree))\n",
        "print(confusion_matrix(y_test, y_pred_tree))"
      ],
      "id": "27168583",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Precision measures how many predicted positives were actually correct. For example, Random Forest had a precision of 0.84 for the positive (before1980) class, meaning when it predicts a home is pre-1980, it is correct 84% of the time. Precision is especially important if a false positive (wrongly classifying a newer home as old) has public health or safety consequences, such as unnecessary asbestos remediation."
      ],
      "id": "860a9a9d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"\\nRandom Forest Evaluation:\")\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "print(confusion_matrix(y_test, y_pred_rf))"
      ],
      "id": "0677b7c6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Recall measures how many true positives were captured among all actual positives. The Random Forest achieved a recall of 0.85 for the positive class, meaning it correctly identified 85% of homes that were truly built before 1980. High recall is crucial if you want to avoid missing any potentially hazardous homes."
      ],
      "id": "e32666d5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"\\nLogistic Regression Evaluation:\")\n",
        "print(classification_report(y_test, y_pred_lr))\n",
        "print(confusion_matrix(y_test, y_pred_lr))"
      ],
      "id": "0dac1b03",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As a balanced measure, the f1-score combines precision and recall, showing the Random Forest at 0.84 for the positive class, which is a solid compromise between missing too many cases and misclassifying safe homes.\n",
        "\n",
        "Interpretation of confusion matrices shows most of the model errors were between these borderline homes built near 1980, which is expected. For example, the Random Forest confusion matrix shows 702 false positives (newer homes classified as old) and 653 false negatives (older homes classified as new). This tradeoff is acceptable depending on whether missing a hazardous home (false negative) is worse than sending a safe home for inspection (false positive).\n",
        "\n",
        "Overall, while no model reached the 90% accuracy target, the Random Forest achieved the best balance of precision, recall, and interpretability, making it the most practical choice for a production environment. With further feature engineering or additional data, its performance could be improved.\n",
        "\n",
        "---\n",
        "\n",
        "## Leakage Check and XGBoost Comparison\n",
        "\n",
        "For this stretch question, I tested three different algorithms to classify whether a home was built before 1980: Random Forest, Logistic Regression, and XGBoost. Each model was evaluated with a confusion matrix and either feature importance or coefficient values.\n",
        "\n",
        "At first, all three models showed perfect or near-perfect accuracy, which seemed too good to be true. After checking, I realized the problem was that I had included yrbuilt as a feature, which is basically the answer to whether a house was built before 1980. Including it let the models ‚Äúcheat‚Äù by memorizing the target, which is why the accuracy was 100%.\n",
        "\n",
        "Random Forest‚Äôs feature importances and XGBoost‚Äôs results both confirmed this, because yrbuilt was by far the most dominant variable. Logistic Regression showed the same thing, with a huge coefficient on yrbuilt.\n",
        "\n",
        "If yrbuilt is removed (which it should be, since you wouldn‚Äôt know it when predicting), Random Forest is still the strongest option. In earlier testing without yrbuilt, it achieved around 80% accuracy with a good balance of precision and recall. That makes it the most reliable recommendation for the client right now, with potential for improvement if more features or neighborhood data are added in the future.\n"
      ],
      "id": "b7dccde4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Stretch Task 1 (corrected - no leakage)\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# load from URL\n",
        "url = \"https://raw.githubusercontent.com/byuidatascience/data4dwellings/master/data-raw/dwellings_ml/dwellings_ml.csv\"\n",
        "joined = pd.read_csv(url)\n",
        "\n",
        "# target\n",
        "y = joined[\"before1980\"]\n",
        "\n",
        "# drop leakage columns (yrbuilt and parcel)\n",
        "X = joined.drop(joined.filter(regex=\"before1980|yrbuilt|parcel\").columns, axis=1)\n",
        "X = X.fillna(0)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "def print_model_results(model, X_test, y_test, feature_names=None):\n",
        "    preds = model.predict(X_test)\n",
        "    cm = confusion_matrix(y_test, preds)\n",
        "    print(f\"\\n{model.__class__.__name__} Confusion Matrix:\")\n",
        "    print(pd.DataFrame(cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Pred 0\", \"Pred 1\"]))\n",
        "    print(classification_report(y_test, preds))\n",
        "    if hasattr(model, \"feature_importances_\"):\n",
        "        fi = pd.Series(model.feature_importances_, index=feature_names)\n",
        "        print(\"Feature Importances:\")\n",
        "        print(fi.sort_values(ascending=False))\n",
        "    elif hasattr(model, \"coef_\"):\n",
        "        coefs = pd.Series(model.coef_[0], index=feature_names)\n",
        "        print(\"Coefficients:\")\n",
        "        print(coefs.sort_values(ascending=False))\n",
        "\n",
        "# 1. Random Forest\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "print_model_results(rf, X_test, y_test, X.columns)\n",
        "\n",
        "# 2. Logistic Regression\n",
        "lr = LogisticRegression(max_iter=1000)\n",
        "lr.fit(X_train, y_train)\n",
        "print_model_results(lr, X_test, y_test, X.columns)\n",
        "\n",
        "# 3. XGBoost\n",
        "xgb = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric=\"logloss\")\n",
        "xgb.fit(X_train, y_train)\n",
        "print_model_results(xgb, X_test, y_test, X.columns)\n"
      ],
      "id": "9c2456cc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Neighborhood Join Without Leakage\n",
        "\n",
        "After merging the neighborhood data with the dwellings data, I reran the same three algorithms: Random Forest, Logistic Regression, and XGBoost. All three models showed perfect or near-perfect accuracy again, with 100% classification rates, which is a strong indicator of data leakage. This happened because the yrbuilt column was still included as a feature after joining, which lets the models essentially memorize the answer.\n",
        "\n",
        "The feature importances and coefficients confirmed this: yrbuilt was still the dominant driver in every model, overwhelming the effect of the new neighborhood variables. The added neighborhood features (like the nbhd_ variables) contributed almost nothing, showing extremely low or even zero importance scores.\n",
        "\n",
        "Because of this, the results with the merged dataset do not actually change the recommended model. If we remove yrbuilt from the features, Random Forest would still likely perform best, similar to its ~80% accuracy from earlier runs. The neighborhood features might provide a small boost if the model is properly cleaned of leakage, but on their own they did not shift the model‚Äôs decision boundaries in a meaningful way.\n",
        "\n",
        "In short, joining the neighborhood data did not meaningfully improve the models when yrbuilt was present, but could be helpful in the future if handled carefully and after removing data leakage.\n"
      ],
      "id": "ba20f6ec"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Stretch Task 2 (corrected - no leakage)\n",
        "\n",
        "# get neighborhood data from URL:\n",
        "neigh_url = \"https://raw.githubusercontent.com/byuidatascience/data4dwellings/master/data-raw/dwellings_neighborhoods_ml/dwellings_neighborhoods_ml.csv\"\n",
        "dwell_url = \"https://raw.githubusercontent.com/byuidatascience/data4dwellings/master/data-raw/dwellings_ml/dwellings_ml.csv\"\n",
        "\n",
        "neigh = pd.read_csv(neigh_url)\n",
        "dwell = pd.read_csv(dwell_url)\n",
        "\n",
        "# merge\n",
        "joined2 = dwell.merge(neigh, on=\"parcel\", how=\"left\")\n",
        "\n",
        "# target\n",
        "y2 = joined2[\"before1980\"]\n",
        "\n",
        "# drop leakage columns\n",
        "X2 = joined2.drop(joined2.filter(regex=\"before1980|yrbuilt|parcel\").columns, axis=1)\n",
        "X2 = X2.fillna(0)\n",
        "\n",
        "X2_train, X2_test, y2_train, y2_test = train_test_split(\n",
        "    X2, y2, test_size=0.2, stratify=y2, random_state=42\n",
        ")\n",
        "\n",
        "# 1. Random Forest\n",
        "rf2 = RandomForestClassifier(random_state=42)\n",
        "rf2.fit(X2_train, y2_train)\n",
        "print_model_results(rf2, X2_test, y2_test, X2.columns)\n",
        "\n",
        "# 2. Logistic Regression\n",
        "lr2 = LogisticRegression(max_iter=1000)\n",
        "lr2.fit(X2_train, y2_train)\n",
        "print_model_results(lr2, X2_test, y2_test, X2.columns)\n",
        "\n",
        "# 3. XGBoost\n",
        "xgb2 = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric=\"logloss\")\n",
        "xgb2.fit(X2_train, y2_train)\n",
        "print_model_results(xgb2, X2_test, y2_test, X2.columns)"
      ],
      "id": "efe93f37",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Additional: Predicting Year Built (Regression)\n",
        "\n",
        "For this stretch question, I built a regression model to predict the year a house was built using a Random Forest Regressor. The model achieved a root mean squared error (RMSE) of about 11.6 years, meaning on average predictions were within roughly 12 years of the true build year. The median absolute error was lower, at around 3.9 years, which shows that half of the predictions were off by less than four years ‚Äî a good sign that the model handles most houses reasonably well, with a few larger outliers.\n",
        "\n",
        "The R¬≤ value was approximately 0.901, which means the model explained about 90% of the variance in the year built. Overall, this is a strong score for a regression problem with a complex target like construction year.\n",
        "\n",
        "While the random forest did a good job predicting year built, there is room to improve by adding more external features, such as neighborhood development data or historical zoning codes, to tighten the RMSE even further. Still, this model would provide a solid starting point for helping the client estimate missing build years when needed.\n"
      ],
      "id": "e8cadd98"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score, median_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Load your confirmed data\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/byuidatascience/data4dwellings/master/data-raw/dwellings_ml/dwellings_ml.csv\")\n",
        "\n",
        "# set target\n",
        "y_reg = df[\"yrbuilt\"]\n",
        "\n",
        "# drop target and identifier\n",
        "X_reg = df.drop(columns=[\"yrbuilt\", \"parcel\"])\n",
        "X_reg = X_reg.fillna(0)\n",
        "\n",
        "# split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_reg, y_reg, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "\n",
        "# predict\n",
        "y_pred = rf_reg.predict(X_test)\n",
        "\n",
        "# evaluation\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))   # fixes the error\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "medae = median_absolute_error(y_test, y_pred)\n",
        "\n",
        "print(f\"RMSE: {rmse:.2f}\")\n",
        "print(f\"R¬≤: {r2:.3f}\")\n",
        "print(f\"Median Absolute Error: {medae:.2f}\")\n",
        "\n",
        "# residual plot\n",
        "residuals = y_test - y_pred\n",
        "plt.scatter(y_pred, residuals)\n",
        "plt.axhline(0, color=\"red\")\n",
        "plt.xlabel(\"Predicted Year Built\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.title(\"Residual Plot\")\n",
        "plt.show()\n",
        "\n",
        "# feature importances\n",
        "pd.Series(rf_reg.feature_importances_, index=X_reg.columns).sort_values().plot(\n",
        "    kind=\"barh\", figsize=(10,12)\n",
        ")\n",
        "plt.title(\"Feature Importances for Year Built Regression\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "63fe51f8",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python312",
      "language": "python",
      "display_name": "Python 3.12 (correct)",
      "path": "/Users/maiafaith/Library/Jupyter/kernels/python312"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}